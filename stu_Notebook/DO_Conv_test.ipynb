{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DO-Conv-test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPOzbRflqgjd7CasUmudJA6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yearing1017/DL_Notes/blob/master/stu_Notebook/DO_Conv_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIXCLyu0GQZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.nn import init\n",
        "from itertools import repeat\n",
        "from torch.nn import functional as F\n",
        "from torch._six import container_abcs\n",
        "from torch._jit_internal import Optional\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class DOConv2d(Module):\n",
        "    \"\"\"\n",
        "       DOConv2d can be used as an alternative for torch.nn.Conv2d.\n",
        "       The interface is similar to that of Conv2d, with one exception:\n",
        "            1. D_mul: the depth multiplier for the over-parameterization.\n",
        "       Note that the groups parameter switchs between DO-Conv (groups=1),\n",
        "       DO-DConv (groups=in_channels), DO-GConv (otherwise).\n",
        "    \"\"\"\n",
        "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
        "                     'padding_mode', 'output_padding', 'in_channels',\n",
        "                     'out_channels', 'kernel_size', 'D_mul']\n",
        "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, D_mul=None, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
        "        super(DOConv2d, self).__init__()\n",
        "        # 网上说 _pair(3)相当于3x3\n",
        "        kernel_size = _pair(kernel_size)\n",
        "        stride = _pair(stride)\n",
        "        padding = _pair(padding)\n",
        "        dilation = _pair(dilation)\n",
        "\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
        "        if padding_mode not in valid_padding_modes:\n",
        "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
        "                valid_padding_modes, padding_mode))\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.groups = groups\n",
        "        self.padding_mode = padding_mode\n",
        "        self._padding_repeated_twice = tuple(x for x in self.padding for _ in range(2))\n",
        "\n",
        "        #################################### Initailization of D & W ###################################\n",
        "        M = self.kernel_size[0]\n",
        "        N = self.kernel_size[1]\n",
        "        self.D_mul = M * N if D_mul is None or M * N <= 1 else D_mul\n",
        "        self.W = Parameter(torch.Tensor(out_channels, in_channels // groups, self.D_mul))\n",
        "        init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
        "\n",
        "        if M * N > 1:\n",
        "            self.D = Parameter(torch.Tensor(in_channels, M * N, self.D_mul))\n",
        "            init_zero = np.zeros([in_channels, M * N, self.D_mul], dtype=np.float32)\n",
        "            self.D.data = torch.from_numpy(init_zero)\n",
        "\n",
        "            eye = torch.reshape(torch.eye(M * N, dtype=torch.float32), (1, M * N, M * N))\n",
        "            D_diag = eye.repeat((in_channels, 1, self.D_mul // (M * N)))\n",
        "            if self.D_mul % (M * N) != 0:  # the cases when D_mul > M * N\n",
        "                zeros = torch.zeros([in_channels, M * N, self.D_mul % (M * N)])\n",
        "                self.D_diag = Parameter(torch.cat([D_diag, zeros], dim=2), requires_grad=False)\n",
        "            else:  # the case when D_mul = M * N\n",
        "                self.D_diag = Parameter(D_diag, requires_grad=False)\n",
        "        ##################################################################################################\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.W)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
        "             ', stride={stride}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if self.bias is None:\n",
        "            s += ', bias=False'\n",
        "        if self.padding_mode != 'zeros':\n",
        "            s += ', padding_mode={padding_mode}'\n",
        "        return s.format(**self.__dict__)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(DOConv2d, self).__setstate__(state)\n",
        "        if not hasattr(self, 'padding_mode'):\n",
        "            self.padding_mode = 'zeros'\n",
        "\n",
        "    def _conv_forward(self, input, weight):\n",
        "        if self.padding_mode != 'zeros':\n",
        "            return F.conv2d(F.pad(input, self._padding_repeated_twice, mode=self.padding_mode),\n",
        "                            weight, self.bias, self.stride,\n",
        "                            _pair(0), self.dilation, self.groups)\n",
        "        return F.conv2d(input, weight, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n",
        "\n",
        "    def forward(self, input):\n",
        "        M = self.kernel_size[0]\n",
        "        N = self.kernel_size[1]\n",
        "        DoW_shape = (self.out_channels, self.in_channels // self.groups, M, N)\n",
        "        if M * N > 1:\n",
        "            ######################### Compute DoW #################\n",
        "            # (input_channels, D_mul, M * N)\n",
        "            D = self.D + self.D_diag\n",
        "            W = torch.reshape(self.W, (self.out_channels // self.groups, self.in_channels, self.D_mul))\n",
        "\n",
        "            # einsum outputs (out_channels // groups, in_channels, M * N),\n",
        "            # which is reshaped to\n",
        "            # (out_channels, in_channels // groups, M, N)\n",
        "            DoW = torch.reshape(torch.einsum('ims,ois->oim', D, W), DoW_shape)\n",
        "            #######################################################\n",
        "        else:\n",
        "            # in this case D_mul == M * N\n",
        "            # reshape from\n",
        "            # (out_channels, in_channels // groups, D_mul)\n",
        "            # to\n",
        "            # (out_channels, in_channels // groups, M, N)\n",
        "            DoW = torch.reshape(self.W, DoW_shape)\n",
        "        return self._conv_forward(input, DoW)\n",
        "\n",
        "\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, container_abcs.Iterable):\n",
        "            return x\n",
        "        return tuple(repeat(x, n))\n",
        "\n",
        "    return parse\n",
        "\n",
        "\n",
        "_pair = _ntuple(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdhwc-URG7ng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39c245a5-33a0-4c83-b44b-467672813d5e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch.nn import functional as F\n",
        "\n",
        "model_urls = {\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "# ResNet中的block类型，指的是1x1,3x3,1x1三种卷积混合的模式，采用先降维再升维，降低计算复杂度\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4 # 在block最后升维的倍数，恢复原来的通道数\n",
        "    # 这里的planes不再是网络中的输出通道数，而是在block中降维的输出通道数\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, conv=DOConv2d, norm=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = norm(planes)\n",
        "        self.conv2 = conv(planes, planes, kernel_size=3, stride=stride,\n",
        "                               dilation=dilation, padding=dilation, bias=False)\n",
        "        self.bn2 = norm(planes)\n",
        "        self.conv3 = conv(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        # 此处的downsample利用1x1卷积来改变通道数，使残差块的连接可以直接相加\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "# deeplabv3的ASPP模块\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, C, depth, num_classes, conv=DOConv2d, norm=nn.BatchNorm2d, momentum=0.0003, mult=1):\n",
        "        super(ASPP, self).__init__()\n",
        "        self._C = C # 进入aspp的通道数\n",
        "        self._depth = depth # filter的个数\n",
        "        self._num_classes = num_classes\n",
        "\n",
        "        self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # 第一个1x1卷积\n",
        "        self.aspp1 = conv(C, depth, kernel_size=1, stride=1, bias=False)\n",
        "        # aspp中的空洞卷积，rate=6，12，18\n",
        "        self.aspp2 = conv(C, depth, kernel_size=3, stride=1,\n",
        "                               dilation=int(6*mult), padding=int(6*mult),\n",
        "                               bias=False)\n",
        "        self.aspp3 = conv(C, depth, kernel_size=3, stride=1,\n",
        "                               dilation=int(12*mult), padding=int(12*mult),\n",
        "                               bias=False)\n",
        "        self.aspp4 = conv(C, depth, kernel_size=3, stride=1,\n",
        "                               dilation=int(18*mult), padding=int(18*mult),\n",
        "                               bias=False)\n",
        "        # 对最后一个特征图进行全局平均池化，再feed给256个1x1的卷积核，都带BN\n",
        "        self.aspp5 = conv(C, depth, kernel_size=1, stride=1, bias=False)\n",
        "        self.aspp1_bn = norm(depth, momentum)\n",
        "        self.aspp2_bn = norm(depth, momentum)\n",
        "        self.aspp3_bn = norm(depth, momentum)\n",
        "        self.aspp4_bn = norm(depth, momentum)\n",
        "        self.aspp5_bn = norm(depth, momentum)\n",
        "        # 先上采样双线性插值得到想要的维度，再进入下面的conv\n",
        "        self.conv2 = conv(depth * 5, depth, kernel_size=1, stride=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = norm(depth, momentum)\n",
        "        # 打分分类\n",
        "        self.conv3 = nn.Conv2d(depth, num_classes, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.aspp1(x)\n",
        "        x1 = self.aspp1_bn(x1)\n",
        "        x1 = self.relu(x1)\n",
        "        x2 = self.aspp2(x)\n",
        "        x2 = self.aspp2_bn(x2)\n",
        "        x2 = self.relu(x2)\n",
        "        x3 = self.aspp3(x)\n",
        "        x3 = self.aspp3_bn(x3)\n",
        "        x3 = self.relu(x3)\n",
        "        x4 = self.aspp4(x)\n",
        "        x4 = self.aspp4_bn(x4)\n",
        "        x4 = self.relu(x4)\n",
        "        x5 = self.global_pooling(x)\n",
        "        x5 = self.aspp5(x5)\n",
        "        x5 = self.aspp5_bn(x5)\n",
        "        x5 = self.relu(x5)\n",
        "        # 上采样：双线性插值使x得到想要的维度\n",
        "        x5 = nn.Upsample((x.shape[2], x.shape[3]), mode='bilinear',\n",
        "                         align_corners=True)(x5)\n",
        "        # 经过aspp之后，concat之后通道数变为了5倍\n",
        "        x = torch.cat((x1, x2, x3, x4, x5), 1)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# 基于ResNet的deeplabv3\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, block_num, num_classes, num_groups=None, weight_std=False, beta=False):\n",
        "        self.inplanes = 64 # 控制残差块的输入通道数 planes:输出通道数\n",
        "        # nn.BatchNorm2d和nn.GroupNorm两种不同的归一化方法\n",
        "        self.norm = lambda planes, momentum=0.5: nn.BatchNorm2d(planes, momentum=momentum) if num_groups is None else nn.GroupNorm(num_groups, planes)\n",
        "        self.conv = DOConv2d\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        if not beta:\n",
        "            # 整个ResNet的第一个conv\n",
        "            self.conv1 = self.conv(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                                   bias=False)\n",
        "        else:\n",
        "            # 第一个残差模块的conv\n",
        "            self.conv1 = nn.Sequential(\n",
        "                self.conv(3, 64, 3, stride=2, padding=1, bias=False),\n",
        "                self.conv(64, 64, 3, stride=1, padding=1, bias=False),\n",
        "                self.conv(64, 64, 3, stride=1, padding=1, bias=False))\n",
        "        self.bn1 = self.norm(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # 建立残差块部分\n",
        "        self.layer1 = self._make_layer(block, 64,  block_num[0])\n",
        "        self.layer2 = self._make_layer(block, 128, block_num[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, block_num[2], stride=2)\n",
        "        # block4开始为dilation空洞卷积\n",
        "        self.layer4 = self._make_layer(block, 512, block_num[3], stride=1, dilation=2)\n",
        "        # aspp,512 * block.expansion是经过残差模块的输出通道数\n",
        "        self.aspp = ASPP(512 * block.expansion, 256, num_classes, conv=self.conv, norm=self.norm)\n",
        "        # 遍历模型进行初始化\n",
        "        for m in self.modules():\n",
        "            #if isinstance(m, self.conv):        #isinstance：m类型判断    若当前组件为 conv\n",
        "                #n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                #m.weight.data.normal_(0, math.sqrt(2. / n))  #正太分布初始化\n",
        "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.GroupNorm): #若为batchnorm\n",
        "                m.weight.data.fill_(1)          #weight为1\n",
        "                m.bias.data.zero_()             #bias为0\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
        "        downsample = None\n",
        "        # stride!=1 代表后续残差块中有stride=2，尺寸大小改变，所以第一个残差块中的stride也该用来修改尺寸\n",
        "        if stride != 1 or dilation != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                self.conv(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, dilation=max(1, dilation/2), bias=False),\n",
        "                self.norm(planes * block.expansion),\n",
        "            )\n",
        "        # laysers 存放产生的残差块，最后根据此列表进行生成网络\n",
        "        layers = []\n",
        "        # 在多个残差块中，只有第一个残差块的输入输出通道不一致，所以先单独添加带downsample的block\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, dilation=max(1, dilation/2), conv=self.conv, norm=self.norm))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, dilation=dilation, conv=self.conv, norm=self.norm))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape:[batch_size, channels, H, w]\n",
        "        size = (x.shape[2], x.shape[3])\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # ASPP\n",
        "        x = self.aspp(x)\n",
        "        x = nn.Upsample(size, mode='bilinear', align_corners=True)(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# 实例化模型\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    # [3,4,6,3]对应block_num,残差块的数量\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=5, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "def resnet101(pretrained=False, num_groups=None, weight_std=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=5,num_groups=num_groups, weight_std=weight_std, **kwargs)\n",
        "    if pretrained:\n",
        "        model_dict = model.state_dict()\n",
        "        if num_groups and weight_std:\n",
        "            pretrained_dict = torch.load('data/R-101-GN-WS.pth.tar')\n",
        "            overlap_dict = {k[7:]: v for k, v in pretrained_dict.items() if k[7:] in model_dict}\n",
        "            assert len(overlap_dict) == 312\n",
        "        elif not num_groups and not weight_std:\n",
        "            pretrained_dict = model_zoo.load_url(model_urls['resnet101'])\n",
        "            overlap_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "        else:\n",
        "            raise ValueError('Currently only support BN or GN+WS')\n",
        "        model_dict.update(overlap_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes=5, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    net = resnet50()\n",
        "    x = torch.rand((4,3,640,640))\n",
        "    output = net(x)\n",
        "    print(output.shape) #(4,5,640,640)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 5, 640, 640])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}