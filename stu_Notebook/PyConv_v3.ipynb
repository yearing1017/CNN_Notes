{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyConv_v3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsT2MdCmtBNTqnpMHMQALt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yearing1017/DL_Notes/blob/master/stu_Notebook/PyConv_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRqfK2Oh7TLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" PyConv networks for image recognition as presented in our paper:\n",
        "    Duta et al. \"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition\"\n",
        "    https://arxiv.org/pdf/2006.11538.pdf\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "class PyConv2d(nn.Module):\n",
        "    \"\"\"PyConv2d with padding (general case). Applies a 2D PyConv over an input signal composed of several input planes.\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input image\n",
        "        out_channels (list): Number of channels for each pyramid level produced by the convolution\n",
        "        pyconv_kernels (list): Spatial size of the kernel for each pyramid level\n",
        "        pyconv_groups (list): Number of blocked connections from input channels to output channels for each pyramid level\n",
        "        stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
        "        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
        "        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``False``\n",
        "    Example::\n",
        "        >>> # PyConv with two pyramid levels, kernels: 3x3, 5x5\n",
        "        >>> m = PyConv2d(in_channels=64, out_channels=[32, 32], pyconv_kernels=[3, 5], pyconv_groups=[1, 4])\n",
        "        >>> input = torch.randn(4, 64, 56, 56)\n",
        "        >>> output = m(input)\n",
        "        >>> # PyConv with three pyramid levels, kernels: 3x3, 5x5, 7x7\n",
        "        >>> m = PyConv2d(in_channels=64, out_channels=[16, 16, 32], pyconv_kernels=[3, 5, 7], pyconv_groups=[1, 4, 8])\n",
        "        >>> input = torch.randn(4, 64, 56, 56)\n",
        "        >>> output = m(input)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, pyconv_kernels, pyconv_groups, stride=1, dilation=1, bias=False):\n",
        "        super(PyConv2d, self).__init__()\n",
        "\n",
        "        assert len(out_channels) == len(pyconv_kernels) == len(pyconv_groups)\n",
        "\n",
        "        self.pyconv_levels = [None] * len(pyconv_kernels)\n",
        "        for i in range(len(pyconv_kernels)):\n",
        "            self.pyconv_levels[i] = nn.Conv2d(in_channels, out_channels[i], kernel_size=pyconv_kernels[i],\n",
        "                                              stride=stride, padding=pyconv_kernels[i] // 2, groups=pyconv_groups[i],\n",
        "                                              dilation=dilation, bias=bias)\n",
        "        self.pyconv_levels = nn.ModuleList(self.pyconv_levels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = []\n",
        "        for level in self.pyconv_levels:\n",
        "            out.append(level(x))\n",
        "\n",
        "        return torch.cat(out, 1)\n",
        "\n",
        "\n",
        "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, groups=1):\n",
        "    \"\"\"standard convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
        "                     padding=padding, dilation=dilation, groups=groups, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class PyConv4(nn.Module):\n",
        "\n",
        "    def __init__(self, inplans, planes, pyconv_kernels=[3, 5, 7, 9], stride=1, pyconv_groups=[1, 4, 8, 16]):\n",
        "        super(PyConv4, self).__init__()\n",
        "        self.conv2_1 = conv(inplans, planes//4, kernel_size=pyconv_kernels[0], padding=pyconv_kernels[0]//2,\n",
        "                            stride=stride, groups=pyconv_groups[0])\n",
        "        self.conv2_2 = conv(inplans, planes//4, kernel_size=pyconv_kernels[1], padding=pyconv_kernels[1]//2,\n",
        "                            stride=stride, groups=pyconv_groups[1])\n",
        "        self.conv2_3 = conv(inplans, planes//4, kernel_size=pyconv_kernels[2], padding=pyconv_kernels[2]//2,\n",
        "                            stride=stride, groups=pyconv_groups[2])\n",
        "        self.conv2_4 = conv(inplans, planes//4, kernel_size=pyconv_kernels[3], padding=pyconv_kernels[3]//2,\n",
        "                            stride=stride, groups=pyconv_groups[3])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat((self.conv2_1(x), self.conv2_2(x), self.conv2_3(x), self.conv2_4(x)), dim=1)\n",
        "\n",
        "\n",
        "class PyConv3(nn.Module):\n",
        "\n",
        "    def __init__(self, inplans, planes,  pyconv_kernels=[3, 5, 7], stride=1, pyconv_groups=[1, 4, 8]):\n",
        "        super(PyConv3, self).__init__()\n",
        "        self.conv2_1 = conv(inplans, planes // 4, kernel_size=pyconv_kernels[0], padding=pyconv_kernels[0] // 2,\n",
        "                            stride=stride, groups=pyconv_groups[0])\n",
        "        self.conv2_2 = conv(inplans, planes // 4, kernel_size=pyconv_kernels[1], padding=pyconv_kernels[1] // 2,\n",
        "                            stride=stride, groups=pyconv_groups[1])\n",
        "        self.conv2_3 = conv(inplans, planes // 2, kernel_size=pyconv_kernels[2], padding=pyconv_kernels[2] // 2,\n",
        "                            stride=stride, groups=pyconv_groups[2])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat((self.conv2_1(x), self.conv2_2(x), self.conv2_3(x)), dim=1)\n",
        "\n",
        "\n",
        "class PyConv2(nn.Module):\n",
        "\n",
        "    def __init__(self, inplans, planes,pyconv_kernels=[3, 5], stride=1, pyconv_groups=[1, 4]):\n",
        "        super(PyConv2, self).__init__()\n",
        "        self.conv2_1 = conv(inplans, planes // 2, kernel_size=pyconv_kernels[0], padding=pyconv_kernels[0] // 2,\n",
        "                            stride=stride, groups=pyconv_groups[0])\n",
        "        self.conv2_2 = conv(inplans, planes // 2, kernel_size=pyconv_kernels[1], padding=pyconv_kernels[1] // 2,\n",
        "                            stride=stride, groups=pyconv_groups[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat((self.conv2_1(x), self.conv2_2(x)), dim=1)\n",
        "\n",
        "\n",
        "def get_pyconv(inplans, planes, pyconv_kernels, stride=1, pyconv_groups=[1]):\n",
        "    if len(pyconv_kernels) == 1:\n",
        "        return conv(inplans, planes, kernel_size=pyconv_kernels[0], stride=stride, groups=pyconv_groups[0])\n",
        "    elif len(pyconv_kernels) == 2:\n",
        "        return PyConv2(inplans, planes, pyconv_kernels=pyconv_kernels, stride=stride, pyconv_groups=pyconv_groups)\n",
        "    elif len(pyconv_kernels) == 3:\n",
        "        return PyConv3(inplans, planes, pyconv_kernels=pyconv_kernels, stride=stride, pyconv_groups=pyconv_groups)\n",
        "    elif len(pyconv_kernels) == 4:\n",
        "        return PyConv4(inplans, planes, pyconv_kernels=pyconv_kernels, stride=stride, pyconv_groups=pyconv_groups)\n",
        "\n",
        "\n",
        "class PyConvBlock(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=None, pyconv_groups=1, pyconv_kernels=1):\n",
        "        super(PyConvBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = get_pyconv(planes, planes, pyconv_kernels=pyconv_kernels, stride=stride,\n",
        "                                pyconv_groups=pyconv_groups)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# deeplabv3的ASPP模块\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, C, depth, num_classes, conv=nn.Conv2d, norm=nn.BatchNorm2d, momentum=0.0003, mult=1):\n",
        "        super(ASPP, self).__init__()\n",
        "        self._C = C # 进入aspp的通道数\n",
        "        self._depth = depth # filter的个数\n",
        "        self._num_classes = num_classes\n",
        "\n",
        "        self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # 第一个1x1卷积\n",
        "        self.aspp1 = conv(C, depth, kernel_size=1, stride=1, bias=False)\n",
        "        # aspp中的空洞卷积，rate=6，12，18\n",
        "        self.aspp2 = conv(C, depth, kernel_size=3, stride=1,\n",
        "                               dilation=int(6*mult), padding=int(6*mult),\n",
        "                               bias=False)\n",
        "        self.aspp3 = conv(C, depth, kernel_size=3, stride=1,\n",
        "                               dilation=int(12*mult), padding=int(12*mult),\n",
        "                               bias=False)\n",
        "        self.aspp4 = conv(C, depth, kernel_size=3, stride=1,\n",
        "                               dilation=int(18*mult), padding=int(18*mult),\n",
        "                               bias=False)\n",
        "        # 对最后一个特征图进行全局平均池化，再feed给256个1x1的卷积核，都带BN\n",
        "        self.aspp5 = conv(C, depth, kernel_size=1, stride=1, bias=False)\n",
        "        self.aspp1_bn = norm(depth, momentum)\n",
        "        self.aspp2_bn = norm(depth, momentum)\n",
        "        self.aspp3_bn = norm(depth, momentum)\n",
        "        self.aspp4_bn = norm(depth, momentum)\n",
        "        self.aspp5_bn = norm(depth, momentum)\n",
        "        # 先上采样双线性插值得到想要的维度，再进入下面的conv\n",
        "        self.conv2 = conv(depth * 5, depth, kernel_size=1, stride=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = norm(depth, momentum)\n",
        "        # 打分分类\n",
        "        self.conv3 = nn.Conv2d(depth, num_classes, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.aspp1(x)\n",
        "        x1 = self.aspp1_bn(x1)\n",
        "        x1 = self.relu(x1)\n",
        "        x2 = self.aspp2(x)\n",
        "        x2 = self.aspp2_bn(x2)\n",
        "        x2 = self.relu(x2)\n",
        "        x3 = self.aspp3(x)\n",
        "        x3 = self.aspp3_bn(x3)\n",
        "        x3 = self.relu(x3)\n",
        "        x4 = self.aspp4(x)\n",
        "        x4 = self.aspp4_bn(x4)\n",
        "        x4 = self.relu(x4)\n",
        "        x5 = self.global_pooling(x)\n",
        "        x5 = self.aspp5(x5)\n",
        "        x5 = self.aspp5_bn(x5)\n",
        "        x5 = self.relu(x5)\n",
        "        # 上采样：双线性插值使x得到想要的维度\n",
        "        x5 = nn.Upsample((x.shape[2], x.shape[3]), mode='bilinear',\n",
        "                         align_corners=True)(x5)\n",
        "        # 经过aspp之后，concat之后通道数变为了5倍\n",
        "        x = torch.cat((x1, x2, x3, x4, x5), 1)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PyConvResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=5, zero_init_residual=False, norm_layer=None, dropout_prob0=0.0):\n",
        "        super(PyConvResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        \n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=2, norm_layer=norm_layer,\n",
        "                                       pyconv_kernels=[3, 5, 7, 9], pyconv_groups=[1, 4, 8, 16])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer,\n",
        "                                       pyconv_kernels=[3, 5, 7], pyconv_groups=[1, 4, 8])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer,\n",
        "                                       pyconv_kernels=[3, 5], pyconv_groups=[1, 4])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer,\n",
        "                                       pyconv_kernels=[3], pyconv_groups=[1])\n",
        "        #self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.aspp = ASPP(512 * block.expansion, 256, num_classes)\n",
        "        if dropout_prob0 > 0.0:\n",
        "            self.dp = nn.Dropout(dropout_prob0, inplace=True)\n",
        "            print(\"Using Dropout with the prob to set to 0 of: \", dropout_prob0)\n",
        "        else:\n",
        "            self.dp = None\n",
        "\n",
        "        #self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, PyConvBlock):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, norm_layer=None, pyconv_kernels=[3], pyconv_groups=[1]):\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        downsample = None\n",
        "        if stride != 1 and self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.MaxPool2d(kernel_size=3, stride=stride, padding=1),\n",
        "                conv1x1(self.inplanes, planes * block.expansion),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "        elif self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "        elif stride != 1:\n",
        "            downsample = nn.MaxPool2d(kernel_size=3, stride=stride, padding=1)\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride=stride, downsample=downsample, norm_layer=norm_layer,\n",
        "                            pyconv_kernels=pyconv_kernels, pyconv_groups=pyconv_groups))\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, norm_layer=norm_layer,\n",
        "                                pyconv_kernels=pyconv_kernels, pyconv_groups=pyconv_groups))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = (x.shape[2], x.shape[3])\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # ASPP\n",
        "        x = self.aspp(x)\n",
        "        x = nn.Upsample(size, mode='bilinear', align_corners=True)(x)\n",
        "\n",
        "        #x = self.avgpool(x)\n",
        "        #x = x.view(x.size(0), -1)\n",
        "\n",
        "        #if self.dp is not None:\n",
        "            #x = self.dp(x)\n",
        "\n",
        "        #x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def pyconvresnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a PyConvResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = PyConvResNet(PyConvBlock, [3, 4, 6, 3], **kwargs)\n",
        " \n",
        "    return model\n",
        "\n",
        "\n",
        "def pyconvresnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a PyConvResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = PyConvResNet(PyConvBlock, [3, 4, 23, 3], **kwargs)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def pyconvresnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a PyConvResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = PyConvResNet(PyConvBlock, [3, 8, 36, 3], **kwargs)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0a6ymPo9E__",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4b155110-105a-4128-a27a-8b40f436e652"
      },
      "source": [
        "net = pyconvresnet152()\n",
        "x = torch.rand((4,3,320,320))\n",
        "for name, layer in net.named_children():\n",
        "    x = layer(x)\n",
        "    print(name, ' output shape:\\t', x.shape) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1  output shape:\t torch.Size([4, 64, 160, 160])\n",
            "bn1  output shape:\t torch.Size([4, 64, 160, 160])\n",
            "relu  output shape:\t torch.Size([4, 64, 160, 160])\n",
            "layer1  output shape:\t torch.Size([4, 256, 80, 80])\n",
            "layer2  output shape:\t torch.Size([4, 512, 40, 40])\n",
            "layer3  output shape:\t torch.Size([4, 1024, 20, 20])\n",
            "layer4  output shape:\t torch.Size([4, 2048, 10, 10])\n",
            "aspp  output shape:\t torch.Size([4, 5, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A-ylDLZ8Ksv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00db2508-5f41-4f19-d9fe-d51842792c9f"
      },
      "source": [
        "    net = pyconvresnet152()\n",
        "    x = torch.rand((4,3,640,640))\n",
        "    output = net(x)\n",
        "    print(output.shape) #(4,5,640,640)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 5, 640, 640])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}